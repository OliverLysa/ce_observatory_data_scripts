{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41d2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose of script: To download wind turbine data from BEIS' Renewable Energy Planning Database and Wikipedia and merge them\n",
    "\n",
    "# Import libraries from which to call functions e.g. urllib \n",
    "import urllib.request\n",
    "import traceback\n",
    "import pandas as pd  # library for data analysis\n",
    "import requests  # library to handle requests\n",
    "from bs4 import BeautifulSoup  # library to parse HTML documents\n",
    "import pyproj\n",
    "import traceback\n",
    "from lat_lon_parser import parse\n",
    "import geopy.distance\n",
    "\n",
    "crs_british = pyproj.Proj(init='EPSG:27700')\n",
    "crs_wgs84 = pyproj.Proj(init='EPSG:4326')\n",
    "\n",
    "words_to_ignore = ['offshore', 'wind', 'farm', 'plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a858b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_commonwords_num(string1, string2, filters):\n",
    "    \"\"\"\n",
    "    :param string1:\n",
    "    :param string2:\n",
    "    :param filters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filters = [x.lower() for x in filters]\n",
    "        list1 = string1.lower().split()\n",
    "        list2 = string2.lower().split()\n",
    "        set2 = set(list2)\n",
    "        f = lambda x: x in set2\n",
    "        return len([x for x in list(filter(f, list1)) if x.lower() not in filters])\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "def convert_uk_grid_to_latlon(row):\n",
    "    \"\"\"\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = row['X-coordinate']\n",
    "        y = row['Y-coordinate']\n",
    "        long, lat = pyproj.transform(crs_british, crs_wgs84, x, y)\n",
    "        return (lat, long)\n",
    "    except Exception as e:\n",
    "        return (0, 0)\n",
    "\n",
    "\n",
    "def formatted_string_to_latlon(geo_line):\n",
    "    \"\"\"\n",
    "    :param geo_line:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lat, lon = geo_line.split('/')[1].strip().split()\n",
    "        lat = parse(lat)\n",
    "        lon = parse(lon)\n",
    "        return (lat, lon)\n",
    "    except Exception as e:\n",
    "        return (0, 0)\n",
    "\n",
    "\n",
    "def get_distance(latlon1, latlon2):\n",
    "    \"\"\"\n",
    "    :param latlon1:\n",
    "    :param latlon2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return round(geopy.distance.geodesic(latlon1, latlon2).km, 2)\n",
    "    except Exception as e:\n",
    "        print(\"Exception in get_distance: {}\".format(str(e)))\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d691f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_df():\n",
    "    \"\"\"\n",
    "    I did almost no changes to your method except extracting a method and hardcode removal + location is still here.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the response in the form of html by calling the URL\n",
    "        wikiurl = \"https://en.wikipedia.org/wiki/List_of_offshore_wind_farms_in_the_United_Kingdom\"\n",
    "        table_class = \"wikitable sortable jquery-tablesorter\"\n",
    "        response = requests.get(wikiurl)\n",
    "        # print response status from call. If 200, then there is success in retrieval of data\n",
    "        print(response.status_code)\n",
    "        # Parse data table from the html into a beautifulsoup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': \"wikitable\"})\n",
    "        df = pd.read_html(str(table))\n",
    "        # Convert that list object to a dataframe\n",
    "        df = pd.DataFrame(df[0])\n",
    "        df.to_csv('pure_wiki.csv', index=False)\n",
    "        # Keep wanted columns in the dataframe\n",
    "        Wikipedia = df[['Name', 'Model', 'Owner', 'Location']]\n",
    "        Wikipedia['Name'] = Wikipedia['Name'].str.strip()\n",
    "        return Wikipedia\n",
    "    except Exception as e:\n",
    "        print(\"Exception in get_wiki_df: {}\".format(str(e)))\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89117c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_REPD_Wind_df():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download Renewable Energy Planning Data (REPD) to location of script by default. Location of save can be specified elsewhere\n",
    "        url = 'https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1096108/repd-july-2022-corrected.csv'\n",
    "        urllib.request.urlretrieve(url, './Renewable_Energy_Planning_Data.csv')\n",
    "\n",
    "        # List of fields to retain from the REPD (REPD), with those not listed to be dropped\n",
    "        fields = ['Ref ID', 'Record Last Updated (dd/mm/yyyy)',\n",
    "                  'Operator (or Applicant)',\n",
    "                  'Site Name',\n",
    "                  'Technology Type',\n",
    "                  'Installed Capacity (MWelec)',\n",
    "                  'Turbine Capacity (MW)',\n",
    "                  'No. of Turbines',\n",
    "                  'Height of Turbines (m)',\n",
    "                  'Development Status',\n",
    "                  'Address',\n",
    "                  'County',\n",
    "                  'Region',\n",
    "                  'Country',\n",
    "                  'Post Code',\n",
    "                  'X-coordinate',\n",
    "                  'Y-coordinate',\n",
    "                  'Operational']\n",
    "\n",
    "        # Read in the REPD csv file, specifying the encoding, while keeping only selected fields\n",
    "        REPD = pd.read_csv(r'Renewable_Energy_Planning_Data.csv', encoding='latin1', usecols=fields)\n",
    "\n",
    "        # Clear column headings of spaces and anything after brackets/parenthesis to make calling them easier e.g. in filtering\n",
    "        REPD.columns = REPD.columns.str.replace(' ', '')\n",
    "        REPD.columns = REPD.columns.str.replace(r\"\\(.*\\)\", \"\")\n",
    "\n",
    "        # Filter to offshore wind in the technology type column\n",
    "        filter_list_technology = ['Wind Offshore']\n",
    "        REPD_Wind = REPD[REPD.TechnologyType.isin(filter_list_technology)]\n",
    "\n",
    "        # Filter to operational in the development status column\n",
    "        filter_list_status = ['Operational']\n",
    "        REPD_Wind = REPD_Wind[REPD_Wind.DevelopmentStatus.isin(filter_list_status)]\n",
    "\n",
    "        # Remove leading and trailing spaces to make it easier to match across data tables\n",
    "        REPD_Wind['SiteName'] = REPD_Wind['SiteName'].str.strip()\n",
    "        return REPD_Wind\n",
    "    except Exception as e:\n",
    "        print(\"Exception in get_REPD_Wind_df: {}\".format(str(e)))\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4effe706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/gszmszx94qn2yq038bgz88mr0000gn/T/ipykernel_76040/416916421.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Wikipedia['Name'] = Wikipedia['Name'].str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "Hywind Scotland Pilot Park (Hywind 2) Demonstrator\n",
      "Hywind Scotland\n",
      "*\n",
      "Beatrice Demonstrator\n",
      "Beatrice\n",
      "*\n",
      "Gunfleet Sands - (Demo) Extension\n",
      "Gunfleet Sands 1 & 2\n",
      "*\n",
      "Gunfleet Sands II\n",
      "Gunfleet Sands 1 & 2\n",
      "*\n",
      "Gunfleet Sands Offshore Wind Scheme\n",
      "Gunfleet Sands 1 & 2\n",
      "*\n",
      "Inner Dowsing\n",
      "Lynn and Inner Dowsing\n",
      "*\n",
      "Lynn\n",
      "Lynn and Inner Dowsing\n",
      "*\n",
      "Robin Rigg East\n",
      "Robin Rigg\n",
      "*\n",
      "Robin Rigg West\n",
      "Robin Rigg\n",
      "*\n",
      "Walney 1\n",
      "Walney\n",
      "*\n",
      "Hornsea 2 - Optimus and Breesea\n",
      "Hornsea One\n",
      "*\n",
      "Kentish Flats 2\n",
      "Kentish Flats\n",
      "*\n",
      "European Offshore Wind Deployment Centre (EOWDC) (Aberdeen Bay - Demonstration site)\n",
      "European Offshore Wind Deployment Centre\n",
      "*\n",
      "Walney 2\n",
      "Walney\n",
      "*\n",
      "Ormonde Offshore\n",
      "Ormonde\n",
      "*\n",
      "Greater Gabbard Wind Farm\n",
      "Greater Gabbard\n",
      "*\n",
      "London Array Phase 1\n",
      "London Array\n",
      "*\n",
      "Centrica (Lincs)\n",
      "Lincs\n",
      "*\n",
      "Teeside Offshore Wind Farm\n",
      "Not found....\n",
      "*\n",
      "Race Bank (Phase 1)\n",
      "Race Bank\n",
      "*\n",
      "Rampion Offshore Wind Farm (Hastings Zone)\n",
      "Rampion\n",
      "*\n",
      "Blyth Offshore Wind Test Site\n",
      "Not found....\n",
      "*\n",
      "East Anglia 1 (EA 1)\n",
      "East Anglia One\n",
      "*\n",
      "Hornsea 1 - Heron & Njord\n",
      "Hornsea One\n",
      "*\n",
      "Galloper Wind Farm\n",
      "Galloper\n",
      "*\n",
      "Levenmouth demonstration turbine (Fife Energy Park)\n",
      "Not found....\n",
      "*\n",
      "Walney 3\n",
      "Walney Extension\n",
      "*\n",
      "Dudgeon East\n",
      "Dudgeon\n",
      "*\n",
      "Burbo Bank Extension (Burbo Bank 2)\n",
      "Burbo Bank\n",
      "*\n",
      "Gwynt y Mor\n",
      "Gwynt y Môr\n",
      "*\n",
      "Humber Gateway A\n",
      "Humber Gateway\n",
      "*\n",
      "Westermost Rough A\n",
      "Westermost Rough\n",
      "*\n",
      "Kincardine Offshore Windfarm\n",
      "Not found....\n",
      "*\n",
      "Race Bank (Phase 2)\n",
      "Race Bank\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    thee main pipeline\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # just extracting the data\n",
    "        Wikipedia = get_wiki_df()\n",
    "        REPD_Wind = get_REPD_Wind_df()\n",
    "        # converting cooords to  lat/lon we canuse  to calculate distance\n",
    "        REPD_Wind['gov_coords'] = REPD_Wind.apply(lambda row: convert_uk_grid_to_latlon(row), axis=1)\n",
    "        Wikipedia['wiki_coords'] = Wikipedia['Location'].apply(formatted_string_to_latlon)\n",
    "\n",
    "        Wikipedia.to_csv('Wiki.csv', index=False)\n",
    "        REPD_Wind.to_csv('REPD_Wind.csv')\n",
    "\n",
    "        # Priority 1:exact matchjooin\n",
    "        merged_left = pd.merge(left=REPD_Wind, right=Wikipedia, how='left', left_on='SiteName', right_on='Name')\n",
    "\n",
    "        # splitting the df into 2 dataframes: with 'pair' after  join (matched)and not matched\n",
    "        # we are going tofind pairsfor 'not matched'\n",
    "        matched = merged_left[merged_left['Name'].str.len() > 0]\n",
    "        not_matched = merged_left[merged_left['Name'].isnull()]\n",
    "        # matched['distance'] = matched.apply(lambda x: get_distance(x['gov_coords'], x['wiki_coords']), axis=1)\n",
    "\n",
    "        series_to_join = []\n",
    "        # iterating non-matched df\n",
    "        for repd_index, repd_row in not_matched.iterrows():\n",
    "            min_distance = 100000\n",
    "            matches_distance = 100000\n",
    "            row_detected = None\n",
    "            match = None\n",
    "            for wiki_index, wiki_row in Wikipedia.iterrows():\n",
    "\n",
    "                # calculating distance with each wiki object\n",
    "                distance = get_distance(repd_row['gov_coords'], wiki_row['wiki_coords'])\n",
    "\n",
    "                # checking for 'inner matches'\n",
    "                if repd_row['SiteName'] in wiki_row['Name'] or wiki_row['Name'] in repd_row['SiteName']:\n",
    "                    if distance < matches_distance:\n",
    "                        match = wiki_row\n",
    "\n",
    "                # checling for ''common words\n",
    "                elif find_commonwords_num(repd_row['SiteName'], wiki_row['Name'], words_to_ignore) > 1:\n",
    "                    if distance < matches_distance:\n",
    "                        match = wiki_row\n",
    "                # update values if it's close\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    row_detected = wiki_row\n",
    "            print('*')\n",
    "            print(repd_row['SiteName'])\n",
    "            # let's check if closest objects haveat least onecommon  word\n",
    "            common_words = find_commonwords_num(repd_row['SiteName'], row_detected['Name'], words_to_ignore)\n",
    "            if common_words == 0 and match is not None:\n",
    "                # it not - lt's use aclosest match\n",
    "                print(match['Name'])\n",
    "                row_to_join = match\n",
    "\n",
    "            elif common_words > 0:\n",
    "                print(row_detected['Name'])\n",
    "                row_to_join = row_detected\n",
    "            else:\n",
    "                print('Not found....')\n",
    "                series_to_join.append(repd_row)\n",
    "                continue\n",
    "            # updating values in initial series\n",
    "            for k in row_to_join.keys():\n",
    "                repd_row[k] = row_to_join[k]\n",
    "            series_to_join.append(repd_row)\n",
    "\n",
    "        # join a list to df and join with amatched df\n",
    "        new_matched = pd.DataFrame(series_to_join)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception in main: {}\".format(str(e)))\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40729ce9",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "        new_matched = matched.append(new_matched)\n",
    "        # filter out some columns\n",
    "        new_matched = new_matched[\n",
    "            [x for x in new_matched.columns if x not in ['Location']]]\n",
    "        new_matched.to_csv(\"final_joined.csv\", index=False)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
